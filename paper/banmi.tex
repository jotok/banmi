%        File: banmi.tex
%     Created: Tue Jan 08 09:00 PM 2013 E
% Last Change: Tue Jan 08 09:00 PM 2013 E
%
\documentclass[letterpaper,11pt]{article}
\usepackage{amsmath,amssymb}
\usepackage[margin=1.2in]{geometry}
\begin{document}

\section{Introduction}

\subsection{The Dirichlet Process}

A Dirichlet Process is a probability distribution on the space of probability measures. Given a space $\mathcal X$, a probability measure $H$ on $\mathcal X$, and $\alpha > 0$ we say that $\nu$ has a Dirichlet Process distribution with parameters $\alpha$ and $H$ and write $\nu \sim DP(\alpha H)$ if the following condition holds. For any disjoint partition $\mathcal X = A_1 \cup \dots \cup A_m$, the random vector $\left( \nu(A_1), \dots, \nu(A_m) \right)$ has a Dirichlet distribution with parameters $\alpha H(A_1)$, \dots, $\alpha H(A_m)$. That is, the vector $(\nu(A_1), \dots, \nu(A_m))$ has a density function proportional to
\begin{align}\label{E:dp-basic}
    \prod_{i=1}^m x^{\alpha H(A_i) - 1}.
\end{align}
One can show that the Dirichlet Process forms a Bayesian conjugate family, and it's been proved that under mild conditions we have posterior consistency for arbitrary populations. The posterior form is simple and intuitive. Let $\xi_1, \dots, \xi_n$ be an i.i.d. sample from $\nu$. If $\nu$ has a $DP(\alpha H)$ prior then
\begin{align}\label{E:dp-posterior}
    \nu | \xi_1, \dots, \xi_n \sim DP\left(\alpha H + \sum_1^n \delta_{\xi_i}\right)
\end{align}
where $\delta_\xi$ denotes a unit mass at $\xi$. To understand \eqref{E:dp-posterior} we should take a closer look at the parameter to the Dirichlet Process, which we originally decomposed into a weight $\alpha > 0$ and a probability measure $H$. From the basic definition \eqref{E:dp-basic} we see that $E [\nu(A)] = H(A)$ for any $A \subset \mathcal X$ and hence $E[\nu] = H$. The weight parameter $\alpha$ governs the variance of $\nu$, and a larger value of $\alpha$ means that we're more likely to draw $\nu$ close to $H$ in some appropriate sense. If we decompose the posterior parameter in \eqref{E:dp-posterior} then we get $\alpha + n$ as the weight parameter and $\frac{\alpha}{\alpha + n}H + \frac{n}{\alpha + n} \left( \frac{1}{n}\sum_1^n \delta_{\xi_i} \right)$ as the mean probability measure. That is, the posterior mean is a weighted combination of $H$ and the empirical measure $\frac{1}{n}\sum_1^n \delta_{\xi_i}$. As $n$ becomes large, the posterior Dirichlet Process puts more and more weight around the empirical distribution.

This brings us to the main difficulty of working with Dirichlet Processes. I have just claimed that the posterior mean is not absolutely continuous (it includes pointmasses at the observed data). In fact, with probability 1 a draw from a Dirichlet Process has countable support. In situations that call for a continuous measure, a simple Dirichlet Process is not appropriate. We can get around this limitation by convolving a realization of the Dirichlet Process with a continuous kernel.

\subsection{Dirichlet Process mixtures}

In the simplest case, a mixture density is a probability density of the form 
\begin{align}\label{E:mixture-simple}
    f(y) = \sum_1^n p_i f(y - y_i; \theta)
\end{align}
where $p_1 + \dots + p_n = 1$ and $\theta$ is some auxiliary parameter. For example, if we take $y_1, \dots, y_n$ to be a sample from an unknown population then a standard density estimator is $\hat{f}(y) = \sum_1^n \varphi(y - y_i; \sigma^2)$, where $\varphi(y; \sigma^2)$ is a Gaussian density with mean 0 and variance $\sigma^2$. More generally, we can replace the $p_i$ by a probability measure $\nu$ called the mixing measure:
\begin{align}
    f(y) = \int f(y - z;\theta) \nu(dz).
    \label{E:mixture-density}
\end{align}
This is sometimes called the convolution of the kernel $f$ with the measure $\nu$. In this setting we can interpret $z$ as a latent variable: if we draw $z \sim \nu$ and $Y \sim f(y - z|\theta)dy$ then the marginal density of $Y$ is given by \eqref{E:mixture-density}.

In this paper I consider mixtures of the form \eqref{E:mixture-density} with a Dirichlet Process prior on $\nu$. Escobar and West described this model in \cite{ew95}, where they give a Gibbs scheme for sampling from the posterior. Let $Y = y_1, \dots, y_n$ be a sample from a population with distribution \eqref{E:mixture-density}. Assume that for each $y_i$ there is a latent variable $z_i$ such that $z_1, \dots, z_n$ are i.i.d. drawn from $\nu$ and $y_i|z_i \sim f(y - z_i; \theta)dy $. Let $\nu \sim DP(\alpha H)$ for some $\alpha$ and $H$, that is, put a Dirichlet Process prior on the mixture measure. Then the density of $y_{n+1} | Y, \theta$ is
\begin{align*}
    f(y|Y, \theta) = \int f(y - z; \theta) \nu(dz | Y, \theta).
\end{align*}
By abuse of notation, $\nu(dz|Y, \theta)$ denotes the posterior mixing measure. In short, the posterior distribution of $y$ is a mixture distribution whose mixture measure is distributed as the posterior of $\nu$.

While $\nu(dz|Y, \theta)$ is generally intractable, we can take advantage of the simple Dirichlet Process posterior as follows. Let $\hat{z}_i = \left\{ z_1, \dots, z_{i-1}, z_{i+1}, \dots, z_n \right\}$. Using the fact that $z_i$ depends on $Y$ only through $y_i$ and applying Bayes' Theorem we obtain
\begin{align}\label{E:mix-posterior-1}
    \nu(dz_i | \hat{z}_i, Y, \theta) = \nu(dz_i | \hat{z}_i, y_i, \theta) \propto f(y_i - z_i; \theta) \nu(z_i | \hat{z}_i)
\end{align}
where $\nu(z_i | \hat{z}_i)$ is given by \eqref{E:dp-posterior}: with probability $\alpha/(\alpha + n - 1)$, $z_i$ is drawn from $H$, and with probability $(n-1)/(\alpha + n - 1)$, $z_i$ is sampled uniformly from $z_1, \dots, z_{i-1}, z_{i+1}, \dots, z_n$. Applying this observation to \eqref{E:mix-posterior-1} we obtain
\begin{align}\label{E:mix-posterior}
    \nu(dz_i|\hat{z}_i, Y, \theta) \propto \alpha f(y_i - z_i; \theta) H(dz_i) + \sum_{i\ne j} f(y_i - z_j; \theta) \delta_{z_j}.
\end{align}
We can thus approximate a draw from $\nu(dz|Y)$ by Gibbs sampling: iteratively update each $z_i$ by drawing a new value from $\nu(dz_i|\hat{z}_j, Y)$.

\section{Data augmentation with Dirichlet Process mixture}

\subsection{Data augmentation}

Define data augmentation and set notation ($Y_{obs}$, $Y_{mis})$ here.

\subsection{A kernel function for mixed categorical and continuous data}

In order to model the joint distribution of variables of which some may be categorical and others continuous, we need a kernel function that takes both categorical and continuous variables as input. Such kernels have been applied by Racine and Li \cite{rl04} to the problem of nonparametric regression, and I follow their approach.

Let $D = D_1 \times \dots \times D_d$ where each $D_j$ is a finite set, and let $d_j = |D_j|$. Assume throughout that each $d_j > 1$. For fixed $j$, $w^j \in D_j$ and $\lambda \in (0, 1)$ define
\begin{align}\label{E:kernel-discrete-1}
    k_j(u^j; w^j, \lambda) = \begin{cases}
        1 - \lambda & u^j = w^j \\
        \frac{\lambda}{d_j - 1} & u^j \ne w^j
    \end{cases}
\end{align}
for $u^j \in D_j$. Then define a kernel function on $D$ by
\begin{align}\label{E:kernel-discrete}
    k(u; w, \lambda) = \prod_{j=1}^d k_j(u^j; w^j, \lambda)
\end{align}
where $u = (u^1, \dots, u^d)$ and $w = (w^1, \dots, w^d)$. I will assume that $\lambda$ is sufficiently small so that $1 - \lambda > \lambda/(d_j - 1)$ for all $j$. Under this assumption, $k$ defines a p.m.f. which is ``centered'' on $w$. The parameter $\lambda$ determines the shape of $k$, with larger values of $\lambda$ giving a more spread out p.m.f.

Next suppose we have data taking values in $\mathcal X = D \times \mathbb R^c$. For $y \in \mathcal X$ write $y = (u, v)$ where $u \in D$ and $v \in \mathbb R^c$. Define the kernel function
\begin{align}\label{E:kernel}
    f(y; w, \mu, \lambda, \sigma) = k(u; w, \lambda) \varphi(v - \mu; \sigma)
\end{align}
where $\mu \in \mathbb R^c$, $\sigma > 0$, and $\varphi$ denotes a Gaussian density with mean 0 and covariance matrix $\sigma^2I$.

Note that the kernel function \eqref{E:kernel} defines a highly symmetric probability measure with independent coordinates. I will rely on the mixture measure to capture the covariance structure of the data. By taking such a simple kernel we make it easy to sample from $k$ conditional on the values of an arbitrary subset of coordinates; this will be convenient when we perform data augmentation.

\subsection{The mixture model}

Let $\mathcal X = D \times \mathbb R^c$ as in the previous section. Let $Y = \left\{ y_1, \dots, y_n \right\} \subset D$ be a sample with values missing that we want to impute using data augmentation. I will model the data using a mixture distribution
\begin{align}\label{E:mix-model}
    f(y) = \int_{\mathcal X} f(y; w, \mu, \lambda, \sigma) \nu(dz)
\end{align}
where $z = (w, \mu)$ and $(y; w, \mu, \lambda, \sigma)$ is given by \eqref{E:kernel}. I will use the following prior distributions:
\begin{align*}
    \mu &\sim DP(\alpha H) \\
    \lambda &\sim \text{Beta}(\alpha_\mu, \beta_\mu) \\
    \sigma^2 &\sim \text{Gamma}^{-1}(\alpha_\sigma, \beta_\sigma)
\end{align*}
where $\alpha$, $H$, $\alpha_\mu$, $\beta_\mu$, $\alpha_\sigma$, and $\beta_\sigma$ are fixed hyperparameters. Introduce latent variables $Z = \left\{ z_1, \dots, z_n \right\}$ which are i.i.d. with common distribution $\mu$ such that $y_i | z_i \sim f(y; z, \lambda, \sigma)dy$.

The main loop of the algorithm will consist of three steps: updating the latent variables, updating $\sigma$ and $\mu$, and drawing new values for the missing data. The updating rule for the latent variables is given by \eqref{E:mix-posterior}.

The distributions of $\lambda | Y, Z$ and $\sigma | Y, Z$ are easily computed from Bayes' rule (note that $\lambda$ and $\sigma$ are independent, so neither appears in the conditioning of the other). Recall that the prior distribution of $\lambda$ is Beta($\alpha_\lambda$, $\beta_\lambda$). We have
\begin{align}\label{E:lambda-posterior-1}
    \pi(\lambda|Y, Z) \propto \lambda^{\alpha_\lambda - 1}(1 - \lambda)^{\beta_\lambda - 1} \prod_{i=1}^n k(y_i ; z_i, \lambda).
\end{align}
Going back to the definition of $k$ in \eqref{E:kernel-discrete-1} and \eqref{E:kernel-discrete},  we see that a term of $1 - \lambda$ will appear on the right-hand side of \eqref{E:lambda-posterior-1} for each (categorical) coordinate of $Y$ that matches its corresponding coordinate in $Z$, and a term of $\lambda$ will appear for each disagreement. Thus $\lambda|Y, Z$ has a beta distribution with parameters $\alpha_\lambda + n_{dis}$ and $\beta_\lambda + n_{agr}$, where $n_{agr}$ counts the total number of agreements in categorical variables between $Y$ and $Z$, and $n_{dis}$ the total number of disagreements. By similar reasoning, and applying the standard conjugacy formula relating the inverse gamma distribution the variance of a normal distribution, we obtain that the distribution of $\sigma^2 | Y, Z$ is an inverse gamma distribution with parameters $\alpha_\sigma + n/2$ and $\beta_\sigma + \sum_1^n \|v_i - \mu_i\|^2 / 2$.

Finally, to update $Y_{mis}$ given $Y_{obs}, Z, \lambda$, and $\sigma$ we simply draw each $y_{i,mis}$ from the density $f(y; z_i, \lambda, \sigma)$ conditioned on the known values of the coordinates. Since $f$ is a product of univariate densities, this is straightforward.

We can summarize the algorithm as follows:

\begin{enumerate}
    \item Set initial values for $Y_{mis}, Z, \sigma$, and $\lambda$.
    \item For $i = 1, \dots, n$ draw a new value for $z_i$:
        \begin{align*}
            z_i \sim \alpha f(z | \hat{z}_i, \lambda, \sigma) H(dz) + \sum_{j \ne i} f(z | \hat{z}_j, \lambda, \sigma) \delta_{z_j}
        \end{align*}
    \item Draw new values for $\lambda$ and $\sigma$:
        \begin{align*}
            \lambda &\sim \text{Beta}\left( \alpha_\lambda + n_{dis}, \beta_\lambda + n_{agr} \right) \\
            \sigma &\sim \text{Gamma}^{-1}\left( \alpha_\sigma + n/2, \beta_\sigma + \sum_1^n \|v_i - \mu_i\|^2/2 \right)
        \end{align*}
    \item For $i = 1, \dots, n$ draw a new value for $y_{i,mis}$ from $f(y ; z_i, \lambda, \sigma)$ conditioned on $y_{i,obs}$
    \item Repeat steps 2--4.
\end{enumerate}

\section{Data augmentation with a mixture copula}

\subsection{The copula model}

A copula model a joint probability model in which the marginal distribution parameters and the covariance structure are estimated separately. I will briefly set up the model and notation I will be using. Smith \cite{smi11} provides a more general introduction to Bayesian copula models.

Due to a theorem of Sklar \cite{skl59}, for any $d$-variate c.d.f. $F$ there exists a function $C : [0, 1]^d \to [0,1]$ such that
\begin{align} \label{E:copula}
    F(y) = C(F_1(y^1), \dots, F_d(y^d)),
\end{align}
where $y = (y^1, \dots, y^d)$ and $F_i$ is the $i$th marginal c.d.f. Such a function $C$ is called a copula function and it captures the covariance structure of the distribution. Conversely, given a distribution function $C$ on the unit cube with uniform marginals and univariate distribution functions $F_1, \dots, F_d$, \eqref{E:copula} defines a $d$-variate distribution function. In both cases, $C$ captures the covariance structure of the distribution.

Part of the power in copula models is the ability to specify unrelated distributions for the margins. For example, we could model $y^1$ as a normal distribution and $y^2$ as a t-distribution. We can choose a model that makes sense for each margin in isolation.

One way to obtain a copula function is through a second invokation of Sklar's theorem. Let $G$ be a $d$-variate distribution function with strictly increasing marginals $G_i$. Then according to \eqref{E:copula},
\begin{align}
    C(u) = G\left( G_1^{-1}(u^1), \dots, G_d^{-1}(u^d) \right)
    \label{E:copula-2}
\end{align}
is a copula function, where $u = (u^1, \dots, u^d) \in [0, 1]^d$. For example, in the Gaussian copula model we take $C(u; \Gamma) =  \Phi\left( \Phi_1^{-1}(u^1), \dots, \Phi_1^{-1}(u^d); \Gamma \right)$, where $\Phi(x; \Gamma)$ is $d$-variate normal distribution function with mean 0 and correlation matrix $\Gamma$, and $\Phi_1$ is the distribution function of the univariate standard normal distribution.

In the next section I will describe two copula models in which the copula function is derived froma mixture of normal distributions.

\begin{thebibliography}{9}
    \bibitem{ew95} Escobar, M. D.. \& West, M. (1995). Baysian density estimation and inference using mixtures. \emph{J. Amer. Statist. Assoc.}, \emph{90}(430), 577--588.

    \bibitem{rl04} Racine, J., \& Li, Q. (2004). Nonaparametric estimation of regression functions with both categorical and continuous data. \emph{J. Econometrics}, \emph{119}(1), 99--130.

    \bibitem{skl59} Sklar, A. (1959) Fonctions de r\'epartition \`a n dimensions et leur marges. \emph{Publications de l'Institut de Statistique de L'Universit de Paris}, \emph{8}, 229--231.

    \bibitem{smi11} Smith, M. S. (2011). Bayesian approaches to copula modelling.
\end{thebibliography}

\end{document}


